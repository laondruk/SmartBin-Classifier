{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOC+ROnUrE3ehXxug+DMby+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavaskiller/smart_bin_proj/blob/main/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J1dZ2p03hu7",
        "outputId": "fda17180-cb64-4daf-aae1-19228a0b0149"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuIMLzXo4dXF"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import metrics, layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, MaxPool2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras import activations\n",
        "from tensorflow.python.keras.layers.pooling import MaxPool2D\n",
        "\n",
        "###################### 변수 #####################\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "img_height = 360\n",
        "img_width = 480\n",
        "learning_rate = 0.001\n",
        "#################################################\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  '/gdrive/My Drive/all',\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size\n",
        "  )\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  '/gdrive/My Drive/all',\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)\n",
        "\n",
        "\n",
        "#데이터 세트 구성 & 데이터 증강\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1. / 255, input_shape=(img_height, img_width, 3)),\n",
        "])\n",
        "\n",
        "augment = Sequential([\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    layers.experimental.preprocessing.RandomContrast(0.3)\n",
        "])\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = (\n",
        "    train_ds.cache()\n",
        "    .shuffle(1000)\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(type(train_ds))\n",
        "\n",
        "#데이터 표준화\n",
        "normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n",
        "\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch  = next(iter(normalized_ds))\n",
        "labels_batch = next(iter(normalized_ds))\n",
        "\n",
        "\n",
        "#모델 만들기\n",
        "num_classes = 5\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "  resize_and_rescale,\n",
        "  augment,\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  Dropout(0.1),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "#모델 컴파일\n",
        "model.compile(optimizer=adam,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#모델 요약\n",
        "model.summary()\n",
        "\n",
        "#모델 훈련\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")\n",
        "\n",
        "#성능 평가\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.suptitle(f\"learning rate: {str(learning_rate)}, batch size: {batch_size}, img size: {img_width} X {img_height}\", fontsize=22)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}